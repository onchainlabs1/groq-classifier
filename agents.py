from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize the model
def get_model():
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        raise ValueError("GROQ_API_KEY not found. Make sure to set the variable in the .env file")
    
    return ChatGroq(
        api_key=api_key,
        model_name="llama3-70b-8192"
    )

# Create a prompt template
system_prompt = """You are an expert agent specialized in answering general questions and also classifying texts into simple categories."""

prompt_template = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{question}")
])

def answer(question: str) -> str:
    """
    Function to answer questions using the Groq LLM model.
    
    Args:
        question (str): The user's question
        
    Returns:
        str: The response generated by the model
    """
    try:
        model = get_model()
        chain = prompt_template | model
        response = chain.invoke({"question": question})
        return response.content
    except Exception as e:
        return f"Error processing the question: {str(e)}" 